#!/usr/bin/env python

from __future__ import print_function
import argparse
import io
import math
import sys

import logging

import morfessorcognate
from morfessorcognate.cognate import CognateModel
from morfessorcognate import CognateConstructionMethods, WILDCARD
from morfessorcognate.data import DataPoint
from morfessorcognate.io import MorfessorIO
from morfessorcognate.corpus import NumMorphCorpusWeight

FIVEDOT = '\u2059' # 5-dot punctuation

def get_argparser():
    parser = argparse.ArgumentParser()
    parser.add_argument('alpha', type=str,
        help='Corpus cost weight. Float or comma-separated pair of floats.')
    parser.add_argument('editweight', type=float,
        help='Edit cost weight')
    parser.add_argument('train_data', type=str,
        help='Path to training data. See data_example.morfessorcognate for format')
    parser.add_argument('textmodel', type=str,
        help='Path to save the model in text format')
    parser.add_argument('binmodel', type=str,
        help='Path to save the model in binary format')
    parser.add_argument('editoutfile', type=str,
        help='Path to dump the final edit counts')
    parser.add_argument('--no-epsilon', dest='noepsilon', action='store_true',
        help='Dont add empty character to end of words')
    parser.add_argument('--num-morph-types', dest='morphtypes', default=None, type=float,
            metavar='<float>',
            help="tune the corpusweight to obtain the desired number of morph "
                 "types")
    parser.add_argument('-d', '--dampening', dest="dampening", type=_str, default='log',
            metavar='<type>', choices=['none', 'log', 'ones'],
            help="frequency dampening for training data ('none', 'log', or "
                 "'ones'; default '%(default)s')")
    return parser

def main(argv):
    parser = get_argparser()
    args = parser.parse_args(argv)
    alpha = args.alpha
    if ',' in alpha:
        alpha_src, alpha_trg = alpha.split(',')
        alpha_src = float(alpha_src)
        alpha_trg = float(alpha_trg)
    else:
        alpha_src = float(alpha)
        alpha_trg = float(alpha)
    ew = args.editweight
    datafile = args.train_data
    textmodel = args.textmodel
    binmodel = args.binmodel
    editoutfile = args.editoutfile
    use_epsilon = not args.noepsilon

    data = []
    mio = MorfessorIO()
    for line in io.open(datafile, encoding='utf-8'):
        count, src, trg = line.strip('\n').split('\t')
        count = int(count)
        if len(src) == 0:
            src = WILDCARD
        elif use_epsilon:
            # append the end epsilon
            src += FIVEDOT
        if len(trg) == 0:
            trg = WILDCARD
        elif use_epsilon:
            # append the end epsilon
            trg += FIVEDOT
        compound = CognateConstructionMethods.type(src, trg)
        if arg.dampening == 'none':
            pass
        elif arg.dampening == 'log':
            count = int(round(math.log(count + 1, 2)))
        elif arg.dampening == 'ones':
            count = 1
        
        data.append(DataPoint(count=count, compound=compound, splitlocs=()))

    model = CognateModel(corpusweight=(alpha_src, alpha_trg),
                         constr_class=CognateConstructionMethods)
    if args.morphtypes:
        # doubled, because num_morph_types returns sum of lexicons
        corpus_weight_updater = NumMorphCorpusWeight(args.morphtypes * 2, threshold=0.05)
        model.set_corpus_weight_updater(corpus_weight_updater)

    model.cost.set_edit_weight(ew)
    model.load_data(data)
    model.train_batch()

    with io.open(textmodel, 'w', encoding='utf-8') as outf:
        for c,_,w in model.get_segmentations():
            print("{} {}".format(c, " + ".join(CognateConstructionMethods.to_string(w1) for w1 in w)), file=outf)
    mio.write_binary_model_file(binmodel, model)

    with io.open(editoutfile, 'w', encoding='utf-8') as outf:
        for w, c in model.cost.edit_cost.counts.most_common():
            if c > 0:
                print('{}\t{}'.format(w, c), file=outf)
    print('alphas: src {} trg {}'.format(model.cost.src_cost._corpus_coding.weight,
                                         model.cost.trg_cost._corpus_coding.weight), file=sys.stderr)
    print('final cost components', file=sys.stderr)
    print('src  {}'.format(model.cost.src_cost.cost()), file=sys.stderr)
    print('trg  {}'.format(model.cost.trg_cost.cost()), file=sys.stderr)
    print('edit {}'.format(model.cost.edit_cost.cost()), file=sys.stderr)
    print('src.lex  {}'.format(model.cost.src_cost._lexicon_coding.get_cost()), file=sys.stderr)
    print('trg.lex  {}'.format(model.cost.trg_cost._lexicon_coding.get_cost()), file=sys.stderr)
    print('edit.lex {}'.format(model.cost.edit_cost._lexicon_coding.get_cost()), file=sys.stderr)
    print('src.cor  {}'.format(model.cost.src_cost._corpus_coding.get_cost()), file=sys.stderr)
    print('trg.cor  {}'.format(model.cost.trg_cost._corpus_coding.get_cost()), file=sys.stderr)
    print('edit.cor {}'.format(model.cost.edit_cost._corpus_coding.get_cost()), file=sys.stderr)
    print('src.lex.bnd  {}'.format(model.cost.src_cost._lexicon_coding.boundaries), file=sys.stderr)
    print('trg.lex.bnd  {}'.format(model.cost.trg_cost._lexicon_coding.boundaries), file=sys.stderr)
    print('edit.lex.bnd {}'.format(model.cost.edit_cost._lexicon_coding.boundaries), file=sys.stderr)
    print('src.cor.bnd  {}'.format(model.cost.src_cost._corpus_coding.boundaries), file=sys.stderr)
    print('trg.cor.bnd  {}'.format(model.cost.trg_cost._corpus_coding.boundaries), file=sys.stderr)
    print('edit.cor.bnd {}'.format(model.cost.edit_cost._corpus_coding.boundaries), file=sys.stderr)
    print('edit.lex.tok {}'.format(model.cost.edit_cost._lexicon_coding.tokens), file=sys.stderr)
    print('edit.lex.logtoksum {}'.format(model.cost.edit_cost._lexicon_coding.logtokensum), file=sys.stderr)

class opts(dict):
    log_file = None
    verbose = 1
    progress = True

if __name__ == "__main__":
    o = opts()
    o['progress'] = True

    morfessorcognate.configure_logger(logging.getLogger(), o)
    main(sys.argv[1:])
